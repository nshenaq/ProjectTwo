title: 'Project 2: Data Mining, Classification, Prediction'
author: "SDS322E"
date: ''
output:
  html_document:
  toc: yes
toc_float:
  collapsed: no
smooth_scroll: yes
pdf_document:
  toc: no
---
  
  ```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
                      tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))
class_diag <- function(score, truth, positive, cutoff=.5){
  pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))
  tab<-table(truth, pred)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[1,1]/rowSums(tab)[1]
  spec=tab[2,2]/rowSums(tab)[2]
  ppv=tab[1,1]/colSums(tab)[1]
  #CALCULATE F1
  f1=2*(sens*ppv)/(sens+ppv)
  
  #CALCULATE EXACT AUC
  truth<-as.numeric(truth=="TRUE")
  ord<-order(score, decreasing=TRUE)
  score <- score[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth))
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(score[-1]>=score[-length(score)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}
```

# Mining, Classification, Prediction

## Noor Shenaq, ns32468

### Introduction 

*The data is called "heart_transplant.csv". The data from this study was obtained from the Journal of the American Statistical Association. The study was conducted by Stanford University for the purpose of seeing if their heart transplant program had a relationship with life expectancy. I chose this study because someone I know recieved a successful heart transplant. It is interesting to asses the survival rate of people with transplants. It might prove to be a promising technological solution. There are a total of 8 variables and 103 observations in this data set:The ID number of the patient, they year of acceptance  as a heart transplant candidate, the age of the patient, the survival status,the survival time, whether the patient had a prior surgery, the transplant status, and the wait time.*           

*We will now be importing the data and the relevant packages for this analysis* 
```{R}

library(tidyverse)
library(dplyr)
library(readr)
library(gt)
data <- read.csv("heart_transplant.csv") %>% select(-wait) %>% na.omit()

data %>% glimpse

```


### Cluster Analysis

*First, we will be loading the necessary packages for this analysis. Then, we will be clustering our data by selecting 3 numeric variables: acceptyear, age, and survtime.We will be determining what is the best number of clusters.* 

```{r}
library(cluster)
library(ggplot2)
library(GGally)
set.seed(322)

#pam1$silinfo$avg.width

#plot(pam1,which=2)
pam_data <- data %>% select(acceptyear, age, survtime)

sil_width<-vector()
for(i in 2:10){  
  pam_fit <- pam(pam_data, k = i)  
  sil_width[i] <- pam_fit$silinfo$avg.width  
}

ggplot()+geom_line(aes(x=1:10,y=sil_width))+scale_x_continuous(name="k",breaks=1:10)



```
*Based on this ggplot, we see that the best amount of clusters is 2*



*We will then select for those 3 variables to prepare the data. * 

```{R}
library(cluster)

clust_data <- data%>%dplyr::select(acceptyear, age, survtime)

``` 
*Next we will be using the PAM function to run the cluster analysis. * 

```{r}
pam1 <-clust_data %>% pam(k=2)
pam1

```

*As we can see above, the mediod is the central most observation. Then we will plot by saving the cluster assignments to the data set and use them to color the points* 

```{r}
pamclust<- clust_data %>% mutate(cluster=as.factor(pam1$clustering))
pamclust %>% ggplot(aes(age,survtime,color=cluster)) + geom_point()

```
*Then we will summarize each cluster* 

```{r}
pamclust %>% group_by(cluster) %>% summarize_if(is.numeric,mean,na.rm=T)
```

*The final mediods are obtained below. They will be representative of their cluster* 

```{r}
data%>%slice(pam1$id.med)
```

*We will determine how good is our cluser solution* 

```{r}


```

*Because the average silhoutte width is between .71 and 1 strong structure has been found*




### Dimensionality Reduction with PCA

*We will now be conducting principle component analysis.It is a procedure that converts many correlated variables into a few uncorrelated variables that are named principal components. We first prepare the data by selecting the numeric data.This standardizes the data *   

```{R}
numeric_data <- data %>% select_if(is.numeric) %>% 
    scale

rownames(numeric_data) <- data$name


```


```{r}
pca_data <- princomp(numeric_data)

names(pca_data)
```

*We then will square to convert SDs to eigenvalues. Next, the proportion of variance explained by each PC is computed. We then run a scree plot to show the variance by each PC. This is to decide how many PCs to keep.* 

```{r}
eigval <- pca_data$sdev^2
varprop = round(eigval/sum(eigval), 2)

ggplot() + geom_bar(aes(y = varprop, x = 1:5), stat = "identity") + 
    xlab("") + geom_path(aes(y = varprop, x = 1:5)) + geom_text(aes(x = 1:5, 
    y = varprop, label = round(varprop, 2)), vjust = 1, col = "white", 
    size = 5) + scale_y_continuous(breaks = seq(0, 0.6, 0.2), 
    labels = scales::percent) + scale_x_continuous(breaks = 1:10)
```
*Based on this plot, 76% of the variance can be explained by the first 2 variables*

*Then, to interpret the PCs we will run the summary function to see the different variables in the PCA and then visualize it in a plot.* 

```{r}
summary(pca_data, loadings = T)
```
```{r}
pca_data$loadings[1:5, 1:2] %>% as.data.frame %>% rownames_to_column %>% 
    ggplot() + geom_hline(aes(yintercept = 0), lty = 2) + geom_vline(aes(xintercept = 0), 
    lty = 2) + ylab("PC2") + xlab("PC1") + geom_segment(aes(x = 0, 
    y = 0, xend = Comp.1, yend = Comp.2), arrow = arrow(), col = "red") + 
    geom_label(aes(x = Comp.1 * 1.1, y = Comp.2 * 1.1, label = rowname))


```



###  Linear Classifier

*We will now look at a logistic regression to predict binary variable. The coefficients were interpreted and the class diagram expression was run*  
```{R}
# linear classifier code here

class_diag<-function(probs,truth){
  
  tab<-table(factor(probs>.5,levels=c("FALSE","TRUE")),truth)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[2,2]/colSums(tab)[2]
  spec=tab[1,1]/colSums(tab)[1]
  ppv=tab[2,2]/rowSums(tab)[2]

  if(is.numeric(truth)==FALSE & is.logical(truth)==FALSE) truth<-as.numeric(truth)-1
  
  #CALCULATE EXACT AUC
  ord<-order(probs, decreasing=TRUE)
  probs <- probs[ord]; truth <- truth[ord]
  
  TPR=cumsum(truth)/max(1,sum(truth)) 
  FPR=cumsum(!truth)/max(1,sum(!truth))
  
  dup<-c(probs[-1]>=probs[-length(probs)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )

  data.frame(acc,sens,spec,ppv,auc)
}

data_5 <- data %>% select(3,4,5,6) %>% mutate(y = ifelse(survived == "alive",1,0))


fit<-glm(y~acceptyear + age + survtime , data=data_5, family="binomial")


#data_5 %>% ggplot(aes(age,survtime)) + geom_point()+geom_smooth(method="lm", se=F)+ylim(0,1) 

score <- predict(fit, type="response")
score %>% round(3)

data_5$prob <-predict(fit,type="response")

fit <- lm(y ~ survtime + age, data=data_5)

class_diag(data_5$prob ,data_5$survived)  


```
*From the model above, we can see that the accuracy is .09708738, the sensitivty is .08, the specifity is .1429571, the ppv is .2. The sensitivity is pretty low which suggests the prescense of false negatives. *



*A confustion matrix was created*

```{r}
table(predict=as.numeric(data_5$prob >.5),truth=data_5$y)%>%addmargins
```



*An ROC plot was created. * 

```{r}
library(plotROC)
ROCplot<-ggplot(data_5)+geom_roc(aes(d=y,m=prob), n.cuts=0) + geom_abline(slope = 1)
ROCplot

```

```{r}
calc_auc(ROCplot)
```
  
*Then we will asses how well this model can generalize a new data set* 
  
```{R}
# cross-validation of linear classifier here

set.seed(1234)
k=10 #choose number of folds
data1<-data_5[sample(nrow(data_5)),] #randomly order rows
folds<-cut(seq(1:nrow(data_5)),breaks=k,labels=F) #create 10 folds
diags<-NULL
for(i in 1:k){
  ## Create training and test sets
  train<-data1[folds!=i,] 
  test<-data1[folds==i,]
  truth<-test$y
  ## Train model on training set
  fit<-glm(y~.,data=train,family="binomial")
  probs<-predict(fit,newdata = test,type="response")
  ## Test model on test set (save all k results)
  diags<-rbind(diags,class_diag(probs,truth))
}
summarize_all(diags,mean) #still no overfitting: CV AUC looks comparable!

```

*After running a k-fold CV, we see that the auc is 1* 

### Non-Parametric Classifier

*Here we will be using the k nearest neighbors method* 
```{R}
library(caret)
# non-parametric classifier code here

knn_fit <- knn3(factor(y==1,levels=c("TRUE","FALSE")) ~ acceptyear + age + survtime, data=data_5, k=5)
y_hat_knn <- predict(knn_fit,data_5)
y_hat_knn

table(truth= factor(data_5$y==1, levels=c("TRUE","FALSE")),
      prediction= factor(y_hat_knn[,1]>.5, levels=c("TRUE","FALSE")))


class_diag(y_hat_knn[,1],data_5$age) 
```

*From the model above, the accuracy is 0.009708738, the senstivity is 0, the spec 1, the ppv is 0, and the auc is 1.*  

*Now we will asses the cross-validation*  
```{R}
# cross-validation of np classifier here

set.seed(1234)
k=10 #choose number of folds
data2<-data_5[sample(nrow(data_5)),] #randomly order rows
folds<-cut(seq(1:nrow(data_5)),breaks=k,labels=F) #create 10 folds
diags<-NULL
for(i in 1:k){
  ## Create training and test sets
  train<-data2[folds!=i,] 
  test<-data2[folds==i,]
  truth<-test$y
  ## Train model on training set
  fit<-knn3(y~.,data=train)
  probs<-predict(fit,newdata = test)[,2]
  ## Test model on test set (save all k results)
  diags<-rbind(diags,class_diag(probs,truth))
}
summarize_all(diags,mean) #CV AUC a smidge worse with kNN
```

*The AUC for the nonparametric model is 0.7766071* 


### Regression/Numeric Prediction

*Lastly, I will fit a regression model to my entire dataset to predict one of my numeric variables from at least 2 other variables* 
```{R}
# regression model code here

lrfit<-lm(age~.,data=data) #predict mpg from all other variables
lryhat<-predict(lrfit) #predicted mpg
mean((data$age-lryhat)^2)


```


*we will then perform k-fold cv and run the class_diagram function*
```{R}
# cross-validation of regression model here

set.seed(1234)
k = 5
data <- data[sample(nrow(data)), ]  #randomly order rows
folds <- cut(seq(1:nrow(data)), breaks = k, labels = F)  #create folds

diags <- NULL
for (i in 1:k) {
    train <- data[folds != i, ]
    test <- data[folds == i, ]
    ## Fit linear regression model to training set
    fit <- lm(age~ ., data = data)
    ## Get predictions/y-hats on test set (fold i)
    yhat <- predict(fit, newdata = test)
    ## Compute prediction error (MSE) for fold i
    diags <- mean((test$age - yhat)^2)
}
mean(diags)

```

*The value for the cross-validation is 39.34759* 


### Python 

*'Reticulate was used below to share objects betwee R and python.The object sweet in R was designed to give the output "iced" . Then the object sweet was also assigned to "Latte"* 
```{R}
library(reticulate)

sweet<- "Iced"

```

*Below is a python code chunk* 
```{python}
# python code here

sweet = "Latte" 

print(r.sweet + sweet) 


```



### Concluding Remarks

*It is interesting to see how diffferent models fit the same data set in different ways and how different selections of data yield different results. This project taught me a lot about modeling datasets to be able to make predictions. It is a useful tool that is applicable in the real world* 


Â© 2021 GitHub, Inc.
Terms
Privacy
Security
Status
Docs
Contact GitHub
Pricing
API
Training
Blog
About
